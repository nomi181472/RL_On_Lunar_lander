{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nomi181472/RL_On_Lunar_lander/blob/main/Hybrid/Hybrid_RL%2BGenetic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGndnx5SQjin"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install swig build-essential  python3-dev xvfb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym gym[box2d] pytorch-lightning pyvirtualdisplay deap"
      ],
      "metadata": {
        "id": "uN3lU3lMRD2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJXz14iiZNBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display(visible=0,size=(1024, 768)).start()"
      ],
      "metadata": {
        "id": "wX2SiiRsRexl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e333833-76dd-46e1-f985-651284643811"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ff84001ca30>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import random"
      ],
      "metadata": {
        "id": "YRwqumTWUp4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor, nn\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "from pytorch_lightning import Trainer, LightningModule"
      ],
      "metadata": {
        "id": "P1zirHOLVZxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics,TimeLimit"
      ],
      "metadata": {
        "id": "z-H9ROtjWHoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3673bf9-3a0b-4d70-a6ac-7b0ae46ee4be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus=torch.cuda.device_count()\n",
        "print(num_gpus)\n",
        "print(device)"
      ],
      "metadata": {
        "id": "CWbZN6rHgXs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(episode=0):\n",
        "\n",
        "  video_file=open(f'/content/videos/rl-video-episode-{episode}.mp4',\"r+b\").read()\n",
        "  video_url=f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'/></video>\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CRgLJE6SgiZw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neural network\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, hidden_state,states,actions):\n",
        "    super().__init__()\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(states,hidden_state,),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_state,hidden_state),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_state,actions)\n",
        "\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.net(x.float())\n",
        "\n"
      ],
      "metadata": {
        "id": "BOVPFZlxth0X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#policy that our agent will follow\n",
        "def epsilon_greedy(state,env,net,epsilon=0.0):\n",
        "  action=-1\n",
        "  if np.random.random()<epsilon:\n",
        "    action=env.action_space.sample()\n",
        "  else:\n",
        "    state=torch.tensor([np.array(state)]).to(device)\n",
        "    q_values=net(state)\n",
        "    max_value,action=torch.max(q_values,dim=1)\n",
        "    action=int(action.item())\n",
        "  return action\n",
        "    #[[s1],[s2]] we are getting highest action for that  states\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OGP7tEXAvPj2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replay memory buffer\n",
        "class ReplayBuffer:\n",
        "  def __init__(self,capacity):\n",
        "    self.buffer=deque(maxlen=capacity)\n",
        "  def __len__(self,):\n",
        "    return len(self.buffer)\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer,batch_size)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MnFWWxDny0kb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "class RLDataset(IterableDataset):\n",
        "  def __init__(self,buffer,sample_size=2000):\n",
        "    self.buffer=buffer\n",
        "    self.sample_size=sample_size\n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vtq99tNG0Kyp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating environment\n",
        "def create_environment(name,):\n",
        "  env=gym.make(name)\n",
        "  env=TimeLimit(env,max_episode_steps=400)\n",
        "  env=RecordVideo(env,video_folder=\"./videos\",episode_trigger=lambda x:x% 50==0)\n",
        "  env=RecordEpisodeStatistics(env) #history of success and failure\n",
        "  return env"
      ],
      "metadata": {
        "id": "5x48QlAX2pAj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env=create_environment(\"LunarLander-v2\")\n",
        "\n",
        "for episode in range(10):\n",
        "  env.reset();\n",
        "  done=False\n",
        "  while not done:\n",
        "    action=env.action_space.sample()\n",
        "    _,_,done,_=env.step(action)\n"
      ],
      "metadata": {
        "id": "Tjuw3qf83Qxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QvnMM0yR3TM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.sample()"
      ],
      "metadata": {
        "id": "xNYoV3Fy4Y80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "class GA:\n",
        "    def __init__(self, hidden_layer, obj_size, n_actions, population_size=100, cross_over_rate=0.1,\n",
        "                 mutate_rate=0.1,\n",
        "\n",
        "                 ):\n",
        "        self.obj_size = obj_size\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.population_size = population_size\n",
        "\n",
        "        self.cross_over_rate = cross_over_rate\n",
        "        self.mutate_rate = mutate_rate\n",
        "        self.populations = []\n",
        "        self.population_performances = []\n",
        "        self.generate_initial_population()\n",
        "\n",
        "    def get_flatten_list(self, index):\n",
        "        flattened_weights = []\n",
        "        model = self.populations[index]\n",
        "        for layer in model.net.children():\n",
        "            if isinstance(layer, torch.nn.Linear):\n",
        "                weights = layer.weight.view(-1).tolist()  # Flatten the weights and convert to a list\n",
        "                flattened_weights.extend(weights)\n",
        "        return flattened_weights\n",
        "\n",
        "    def crossover(self, parent1, parent2):\n",
        "        child = []\n",
        "        for i in range(len(parent1)):\n",
        "            if i % 2 == 0:\n",
        "                child.append(parent2[i])\n",
        "            else:\n",
        "                child.append(parent1[i])\n",
        "        return child\n",
        "\n",
        "    def mutate(self, child):\n",
        "        for i in range(int(len(child) * self.mutate_rate)):\n",
        "            index = np.random.randint(0, len(child) - 1)\n",
        "            child[index] = np.random.normal()\n",
        "        return child\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def mating(self, ):\n",
        "        # Get the top 10 maximum numbers with indices using a heap\n",
        "        selection = 76\n",
        "        top_max = heapq.nlargest(selection, enumerate(self.population_performances), key=lambda x: x[1])\n",
        "        top_indices = [index for index, value in top_max]\n",
        "        for i in range(int(selection / 2)):\n",
        "            # Todo selection\n",
        "            first_parent_index = random.choice(top_indices)\n",
        "            top_indices.remove(first_parent_index)\n",
        "            second_parent_index = random.choice(top_indices)\n",
        "            top_indices.remove(second_parent_index)\n",
        "            # Todo flatten\n",
        "            first_parent = self.get_flatten_list(first_parent_index)\n",
        "            second_parent = self.get_flatten_list(second_parent_index)\n",
        "            # Todo Crossover\n",
        "            child = self.crossover(first_parent, second_parent)\n",
        "            # Todo Mutation\n",
        "            child = self.mutate(child)\n",
        "            # Todo child expected score\n",
        "            child_expected_score = (self.population_performances[first_parent_index] + self.population_performances[\n",
        "                second_parent_index]) / 2\n",
        "            # Todo replace worst solution\n",
        "            worst_solution = min(self.population_performances)\n",
        "            worst_solution_index = self.population_performances.index(worst_solution)\n",
        "            self.population_performances[worst_solution_index] = child_expected_score\n",
        "            model = self.populations[worst_solution_index]\n",
        "            # Todo back to list\n",
        "            index = 0\n",
        "            for layer in model.net.children():\n",
        "                if isinstance(layer, torch.nn.Linear):\n",
        "                    # Calculate the number of weights in the current layer\n",
        "                    num_weights = layer.weight.numel()\n",
        "                    # Extract the weights from the combined list and reshape them\n",
        "                    new_weights = torch.tensor(child[index:index + num_weights]).view(layer.weight.size())\n",
        "                    # Assign the modified weights back to the layer\n",
        "                    layer.weight.data = new_weights\n",
        "                    # Move to the next set of weights in the combined list\n",
        "                    index += num_weights\n",
        "\n",
        "            self.populations[worst_solution_index]=model\n",
        "\n",
        "    def generate_initial_population(self):\n",
        "        for i in range(self.population_size):\n",
        "            self.populations.append(DQN(self.hidden_layer, self.obj_size, self.n_actions))  # qtable)\n",
        "            # model = DQN(self.hidden_layer, self.obj_size, self.n_actions)\n",
        "            # flattened_weights = []\n",
        "            # # for layer in model.net.children():\n",
        "            # #     if isinstance(layer, torch.nn.Linear):\n",
        "            # #         weights = layer.weight.view(-1).tolist()  # Flatten the weights and convert to a list\n",
        "            # #         flattened_weights.extend(weights)\n",
        "            # # self.populations.append(flattened_weights)\n",
        "            # for idx, layer in enumerate(model.net.children()):\n",
        "            #     if isinstance(layer, torch.nn.Linear):\n",
        "            #         input_size = layer.in_features\n",
        "            #         output_size = layer.out_features\n",
        "            #         print(f\"Layer {idx + 1}: Input Size: {input_size}, Output Size: {output_size}\")\n",
        "\n",
        "\n",
        "\n",
        "class DeepQLearning(LightningModule):\n",
        "  #initialize\n",
        "  def __init__(self,\n",
        "               env_name, policy=epsilon_greedy,capacity=100_000,\n",
        "               batch_size=256, lr=1e-3,hidden_layer=128,gamma=0.99,\n",
        "               loss_fn=F.smooth_l1_loss,optim=AdamW,eps_start=0,\n",
        "               eps_end=0.15,eps_last_episode=100,sample_per_epoch=10_000,sync_rate=100,\n",
        "               max_iters=700\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.env=create_environment(env_name)\n",
        "    obj_size=env.observation_space.shape[0]\n",
        "    n_actions=env.action_space.n\n",
        "    self.GA = GA(hidden_layer, obj_size, n_actions)\n",
        "    self.q_net = self.get_max_performer()\n",
        "    #self.q_net=DQN(hidden_layer,obj_size,n_actions)\n",
        "    self.target_q_net=copy.deepcopy(self.q_net)\n",
        "    self.policy=policy\n",
        "    self.max_reward=0\n",
        "    self.max_iters=max_iters\n",
        "    self.buffer=ReplayBuffer(capacity=capacity)\n",
        "    self.save_hyperparameters()\n",
        "    while len(self.buffer)<self.hparams.sample_per_epoch:\n",
        "      print(f\"{len(self.buffer)} sampes in experieces buffer filling.... \")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def get_max_performer(self):\n",
        "      index = 0\n",
        "      highest_reward = float('-inf')\n",
        "      for idx, solution in enumerate(self.GA.populations):\n",
        "          solution.train()\n",
        "          state = self.env.reset()\n",
        "          state = torch.tensor(state, dtype=torch.float32)\n",
        "          total_reward = 0\n",
        "          for i in range(700):\n",
        "              # Compute prediction error\n",
        "\n",
        "              pred = solution(state)\n",
        "              action = torch.argmax(pred).item()\n",
        "\n",
        "              next_state, reward, done, info = self.env.step(action)\n",
        "              next_state = torch.tensor(next_state, dtype=torch.float32)\n",
        "              total_reward = total_reward + reward\n",
        "              state = next_state\n",
        "              if done:\n",
        "                  break\n",
        "          if total_reward > highest_reward:\n",
        "              highest_reward = total_reward\n",
        "              index = idx\n",
        "          self.GA.population_performances.append(total_reward)\n",
        "      return self.GA.populations[index]\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episode(self,policy=None,epsilon=0.):\n",
        "    #print(\"play_episode\")\n",
        "    state=self.env.reset()\n",
        "    done =False\n",
        "    action=2\n",
        "    while not done:\n",
        "\n",
        "      if policy:\n",
        "        action=policy(state,self.env,self.q_net,epsilon=epsilon)\n",
        "        #print(f\"policy available, action: {action} {policy}\")\n",
        "      else:\n",
        "        action=self.env.action_space.sample()\n",
        "        #print(f\"policy available, action: {action}\")\n",
        "      next_state,reward,done,info=self.env.step(action)\n",
        "      exp=(state,action,reward,done,next_state)\n",
        "      self.buffer.append(exp)\n",
        "      state=next_state\n",
        "    #print(\"play_episode-done\")\n",
        "\n",
        "\n",
        "  #forward\n",
        "  def forward(self,x):\n",
        "    #print(\"forward\")\n",
        "    return self.q_net(x);\n",
        "\n",
        "  #configure Optimizer\n",
        "  def configure_optimizers(self):\n",
        "    #print(\"configure_optimizers\")\n",
        "    q_net_optimizer=self.hparams.optim(self.q_net.parameters(),lr=self.hparams.lr)\n",
        "    #print(\"configure_optimizers-done\")\n",
        "    return [q_net_optimizer]\n",
        "  def train_dataloader(self):\n",
        "    #print(\"train_dataloader\")\n",
        "    dataset=RLDataset(self.buffer,self.hparams.sample_per_epoch);\n",
        "    dataloader=DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size\n",
        "    )\n",
        "    #print(\"train_dataloader-done\")\n",
        "    return dataloader\n",
        "  #training steps\n",
        "  def training_step(self,batch,batch_ids):\n",
        "    #print(\"training_step\")\n",
        "\n",
        "    states,actions,rewards,dones,next_states=batch\n",
        "    actions=actions.unsqueeze(1)\n",
        "    rewards=rewards.unsqueeze(1)\n",
        "    dones=dones.unsqueeze(1)\n",
        "\n",
        "    state_action_values=self.q_net(states).gather(1,actions)\n",
        "\n",
        "    next_action_values,_=self.target_q_net(next_states).max(dim=1,keepdim=True)\n",
        "    next_action_values[dones]=0.0\n",
        "\n",
        "    expected_state_action_values=rewards +self.hparams.gamma * next_action_values\n",
        "\n",
        "    loss= self.hparams.loss_fn(state_action_values,expected_state_action_values)\n",
        "    self.log(\"episode/Q-error\",loss)\n",
        "    #print(\"training_step-done\")\n",
        "    return loss\n",
        "  #training epoch end\n",
        "  def on_train_epoch_end(self):\n",
        "    #print(\"on_train_epoch_end\")\n",
        "\n",
        "    epsilon=max(self.hparams.eps_end,self.hparams.eps_start-self.current_epoch/self.hparams.eps_last_episode)\n",
        "    self.play_episode(policy=self.policy,epsilon=epsilon)\n",
        "    self.log(\"episode/return\",self.env.return_queue[-1])\n",
        "    if self.env.return_queue[-1]>self.max_reward:\n",
        "      self.max_reward=self.env.return_queue[-1]\n",
        "    self.GA.mating()\n",
        "    print(f\"Epoch:{self.current_epoch}: Max Reward:{self.max_reward}\")\n",
        "    if self.current_epoch %self.hparams.sync_rate==0:\n",
        "      self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "    #print(\"on_train_epoch_end-done\")\n"
      ],
      "metadata": {
        "id": "cogUzPKZ4ghk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ],
      "metadata": {
        "id": "5b-HvVhZPCrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algo=DeepQLearning(\"LunarLander-v2\")\n"
      ],
      "metadata": {
        "id": "53CLhlmlDQKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=Trainer(accelerator=\"cuda\",max_epochs=10_000,\n",
        "                callbacks=[EarlyStopping(monitor=\"episode/return\",mode=\"max\",patience=500)])\n",
        "trainer.fit(algo)"
      ],
      "metadata": {
        "id": "TVCZW3VqDcO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_video(800)"
      ],
      "metadata": {
        "id": "e8D-mzCyLHPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjVGcWHFZOyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}